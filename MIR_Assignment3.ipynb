{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bbdb28",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "- In this assignment, you will implement a melody-language model\n",
    "- You have to submit your code in **TWO** formats:\n",
    "    - Completed Notebook with `.ipynb`\n",
    "    - A `{your_student_number}.py` file that includes **ALL functions and classes you have completed**\n",
    "        - Do not include any other code except function and class\n",
    "        - Your result will be scored by an evaluation code that import this `{your_student_number}.py` file\n",
    "        - So be careful not to use any global variable inside the function\n",
    "- You have to submit a report (optional) and **three** generation results of your favorite in wav files\n",
    "    - The report is optional. If you have tried other architecture for MelodyLanguage Model, you can describe the result.\n",
    "\n",
    "\n",
    "- Caution: The `assert` lines are designed to check whether basic requirements are satisfied. Even though you passed all the assert cases, it doesn't guarantee that your implementation is fully correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d255bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24cd5c",
   "metadata": {},
   "source": [
    "## 0. Prepare (Install and import library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e670ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install muspy\n",
    "import muspy\n",
    "\n",
    "muspy.download_bravura_font()\n",
    "'''\n",
    "You may have to install fluidsynth.\n",
    "In Colab, you can install by followign code\n",
    "\n",
    "!sudo apt-get install fluidsynth\n",
    "'''\n",
    "muspy.download_musescore_soundfont()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9b3ba",
   "metadata": {},
   "source": [
    "## Problem 1: Understanding and Implementing RNN (15 pts)\n",
    "- Recurrent neural network is a typical choice for handling sequential data with a neural network\n",
    "- In this problem, you have to implement a Vanilla RNN\n",
    "    - For each time step $t$, RNN takes two inputs\n",
    "        - $x_t$, which is an input vector of time step $t$\n",
    "        - $h_{t-1}$, which is an hidden state of previous time step, $t-1$\n",
    "            - $h_{t-1}$ is also the output of RNN for previous time step $t-1$\n",
    "    - For given $x_t$ and $h_{t-1}$, RNN returns $h_t$\n",
    "        - $h_t = \\tanh(Wx_t + Uh_{t-1} + b)$ \n",
    "            - $W$ and $U$ is a trainable weight matrix of RNN\n",
    "            - $W \\in \\mathbb{R}^{d \\times h}$, and $U \\in \\mathbb{R}^{h \\times h}$, where $d$ is number of input dimension and $h$ is number of hidden state dimension\n",
    "                - This means that $W$ is a matrix with real numbers and size of $\\text{num_input_dim}\\times \\text{num_hidden_dim}$  \n",
    "                - and $U$ is a matrix with real numbers and size of $\\text{num_hidden_dim}\\times \\text{num_hidden_dim}$  \n",
    "            \n",
    " - The output of fully connected layer (`nn.Linear`) for a given input vector $x$ is as below:\n",
    "     - $\\text{output} = Wx+b$\n",
    "     - Where $W$ is a weight matrix and $b$ is a bias vector\n",
    "     - Both $W$ and $b$ are trainable parameters\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050d35e",
   "metadata": {},
   "source": [
    "### Problem 1.1: Calculating Forward Propagation of RNN\n",
    "- Based on the example above, implement the forward propagation of uni-directional, single layer vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Don't change this cell\n",
    "'''\n",
    "example_input_size = 3\n",
    "example_hidden_size = 6\n",
    "example_sequence_length = 20\n",
    "\n",
    "torch.manual_seed(0)\n",
    "example_weight_for_hidden_to_hidden = torch.randn([example_hidden_size, example_hidden_size])\n",
    "example_weight_for_input_to_hidden = torch.randn([example_hidden_size, example_input_size])\n",
    "example_bias = torch.randn([example_hidden_size])\n",
    "example_input_sequence = torch.randn([example_sequence_length, example_input_size])\n",
    "\n",
    "print('example_weight_for_hidden_to_hidden: \\n',example_weight_for_hidden_to_hidden)\n",
    "print('example_weight_for_input_to_hidden: \\n',example_weight_for_input_to_hidden)\n",
    "print('example_bias: \\n',example_bias)\n",
    "print('example_input_sequence: \\n',example_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_single_step(current_input:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> torch.Tensor:\n",
    "  '''\n",
    "  This function \n",
    "  \n",
    "  Arguments:\n",
    "    current_input: Input vector of the current time step. Has a shape of [input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "    bias: Bias of RNN. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  Outputs:\n",
    "    current hidden: Updated hidden state for the current time step. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  return \n",
    "\n",
    "\n",
    "def initialize_hidden_state_for_single_batch(hidden_dim:int) -> torch.Tensor:\n",
    "  '''\n",
    "  This function returns zero Tensor for a given hidden dimension. This function assumes that the RNN uses single layer and single direction.\n",
    "  \n",
    "  Argument\n",
    "    hidden_dim\n",
    "    \n",
    "  Return\n",
    "    initial_hidden_state: Has a shape of [hidden_dim]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  return \n",
    "\n",
    "\n",
    "initial_hidden = initialize_hidden_state_for_single_batch(example_hidden_size)\n",
    "assert initial_hidden.shape == torch.Size([example_hidden_size])\n",
    "\n",
    "single_output = rnn_single_step(example_input_sequence[0], initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)\n",
    "assert (torch.abs(single_output - torch.Tensor([ 0.2690, -0.9982,  0.9929, -0.9535,  1.0000,  0.0081]))<1e-4).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_for_entire_timestep(input_seq:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> tuple:\n",
    "  '''\n",
    "  This function returns the output of RNN for the given 'input_seq', for the given RNN's parameters (hh_weight, ih_weight, and bias)\n",
    "  \n",
    "  Arguments:\n",
    "    input_seq: Sequence of input vector. Has a shape of [number_of_timestep, input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "\n",
    "  \n",
    "  Return: tuple (output, final_hidden_state)\n",
    "    output: Sequence of output hidden state of RNN along input timesteps. Has a a shape of [number_of_timestep, hidden_dimension]\n",
    "    final_hidden_state: Hidden state of RNN of the last time step. Has a a shape of [hidden_dimension]\n",
    "    \n",
    "  TODO: Complete this function using your 'rnn_single_step()'\n",
    "  '''\n",
    "  \n",
    "  return\n",
    "\n",
    "total_output = rnn_for_entire_timestep(example_input_sequence, initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6557792",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test case\n",
    "'''\n",
    "\n",
    "assert (total_output[0][6] - torch.Tensor([ 0.8273,  0.5121, -0.5701, -0.9566,  0.9984,  0.5125])).abs().min() < 1e-4, f\"Output value is different: {total_output[0][6]}\"\n",
    "assert (total_output[1]- torch.Tensor([-0.2121, -0.9892, -0.9953,  0.7993,  1.0000, -0.9995])).abs().min() < 1e-4, f\"Output value is different: {total_output[1]}\"\n",
    "\n",
    "print(\"Passed the test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3661e",
   "metadata": {},
   "source": [
    "## Problem 2: Understanding Embedding Layer (10 pts)\n",
    "- Embedding Layer takes categorical indices and return corresponding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc07eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbeddingLayer(nn.Module):\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.weight = torch.randn(num_embeddings, embedding_dim)\n",
    "    \n",
    "  def forward(self, x:torch.LongTensor):\n",
    "    '''\n",
    "    Argument\n",
    "      x: torch.LongTensor of arbitrary shape, where each element represent categorical index smaller than self.num_embeddings\n",
    "      \n",
    "    Return\n",
    "      out: torch.FloatTensor with [shape of x, self.embedding_dim]\n",
    "    \n",
    "    TODO: Complete this function using self.weight\n",
    "    '''\n",
    "    \n",
    "    return\n",
    "  \n",
    "custom_embedding_layer = CustomEmbeddingLayer(10, example_input_size)\n",
    "random_categorical_input = torch.randint(0,10, [3, 2, 2])\n",
    "random_categorical_input, custom_embedding_layer(random_categorical_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1b7d2",
   "metadata": {},
   "source": [
    "## Problem 3: Dataset (20 pts)\n",
    "- You have to declare a path for saving dataset\n",
    "- The dataset has vocabulary information\n",
    "    - For both pitch and duration, we added `'start'` and `'end'` token\n",
    "    - This helps a language model to start the generation or end the generation\n",
    "- You have to implment `__getitem__` of this dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b466ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_path = 'essen_folk/'\n",
    "'''\n",
    "You can download the dataset like this, but it will take too much time in Colab\n",
    "\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "'''\n",
    "# !pip install --upgrade gdown\n",
    "!gdown 1HMHgPifMFgRtIiLJsTb3ULqbxJx4xpQY # If it doesn't work, you have to upgrade gdown by !pip install --upgrade gdown\n",
    "\n",
    "# Following code will automatically unzip te dataset to essen_folk/\n",
    "!unzip -oq essen_converted.zip  # option: overwrite, quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyDataset:\n",
    "  def __init__(self, muspy_dataset, vocabs=None):\n",
    "    self.dataset = muspy_dataset\n",
    "    \n",
    "    if vocabs is None:\n",
    "      self.idx2pitch, self.idx2dur = self._get_vocab_info()\n",
    "      self.idx2pitch += ['start', 'end']\n",
    "      self.idx2dur += ['start', 'end']\n",
    "      self.pitch2idx = {x:i for i, x in enumerate(self.idx2pitch)}\n",
    "      self.dur2idx = {x:i for i, x in enumerate(self.idx2dur)}\n",
    "      \n",
    "    else:\n",
    "      self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    \n",
    "  def _get_vocab_info(self):\n",
    "    entire_pitch = []\n",
    "    entire_dur = []\n",
    "    for note_rep in self.dataset:\n",
    "      pitch_in_piece = note_rep[:, 1]\n",
    "      dur_in_piece = note_rep[:, 2]\n",
    "      entire_pitch += pitch_in_piece.tolist()\n",
    "      entire_dur += dur_in_piece.tolist()\n",
    "    return list(set(entire_pitch)), list(set(entire_dur))\n",
    "  \n",
    "  def get_vocabs(self):\n",
    "    return self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    '''\n",
    "    This dataset class returns melody information as a tensor with shape of [num_notes, 2 (pitch, duration)].\n",
    "    \n",
    "    To train a melody language model, you have to provide a sequence of original note, and a sequence of next note for given original note.\n",
    "    In other word, melody[i+1] has to be the shifted_melody[i], so that melody[i]'s next note can be retrieved by shifted_melody[i]\n",
    "    (Remember, language model is trained to predict the next upcoming word)\n",
    "    \n",
    "    Also, to make genration easier, we usually add 'start' token at the beginning of sequence, and 'end' token at the end of the sequence.\n",
    "    With these tokens, we can make the model recognize where is the start and end of the sequence explicitly.\n",
    "    \n",
    "    You have to add these tokens to the note sequence at this step.\n",
    "    \n",
    "    Argument:\n",
    "      idx (int): Index of data sample in the dataset\n",
    "    \n",
    "    Returns:\n",
    "      melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                 Has a shape of [1 (start_token) + num_notes, 2 (pitch, dur)]. \n",
    "                                 The first element of the sequence has to be the index for 'start' token for both pitch and duration.\n",
    "                                 The melody should not include 'end' token (Because we don't have to predict next note if we know that current note is 'end' token)\n",
    "      shifted_melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                         Has a shape of [num_notes + 1 (end_token), 2 (pitch, dur)]\n",
    "                                         The i'th note of shifted melody has to be the same with (i+1)'th note of melody\n",
    "                                         The shifted melody should not include 'start' token \n",
    "                                         (Because we never get a 'start' token after a note)\n",
    "\n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "    \n",
    "    return\n",
    "\n",
    "your_path = 'essen_folk/'\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "\n",
    "essen_entire = essen.to_pytorch_dataset(representation='note')\n",
    "essen_split = essen.to_pytorch_dataset(representation='note', splits=(0.8, 0.1, 0.1), random_state=0)\n",
    "entire_set = MelodyDataset(essen_entire)\n",
    "\n",
    "train_set = MelodyDataset(essen_split['train'], vocabs=entire_set.get_vocabs())\n",
    "valid_set = MelodyDataset(essen_split['validation'], vocabs=entire_set.get_vocabs())\n",
    "test_set = MelodyDataset(essen_split['test'], vocabs=entire_set.get_vocabs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0372bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To check the MelodyDataset implementation\n",
    "'''\n",
    "\n",
    "assert len(train_set[0]) == 2, \"You have to return two variables at __getitem__\"\n",
    "assert train_set[0][0].shape == train_set[0][1].shape, \"Shape of Melody and Shifted melody has to be the same\"\n",
    "\n",
    "assert (train_set[0][0][0] == torch.LongTensor([38, 44])).all(), \"You have to add start token at the beginning of melody\"\n",
    "assert (train_set[0][1][-1] == torch.LongTensor([39, 45])).all(), \"You have to add end token at the end of melody\"\n",
    "\n",
    "assert (train_set[0][0][-1] == torch.LongTensor([12, 26])).all(), \"Last part of melody must not include the end token\"\n",
    "assert (train_set[0][1][0] == torch.LongTensor([24, 16])).all(),  \"First part of shifted melody must not include the start token\"\n",
    "\n",
    "assert (train_set[20][0][1:] == train_set[20][1][:-1]).all(), \"Check the melody shift\"\n",
    "\n",
    "print(\"Passed test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673c342",
   "metadata": {},
   "source": [
    "## PackSequence\n",
    "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
    "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
    "- In this problem, you will learn about how to handle sequences of different length as a batch\n",
    "\n",
    "- You can also refer [a video lecture](https://youtu.be/IQf1zu6jdCU) in Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83754613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''\n",
    "This cell will make error, because the length of each sample is different to each other\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8)\n",
    "batch = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To handle that problem, you have to make your collate function \n",
    "'''\n",
    "def your_collate_function(raw_batch):\n",
    "  '''\n",
    "  You can make your own function to handle the batch\n",
    "  '''\n",
    "  \n",
    "  return raw_batch[0] # This returns the first melody of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
    "\n",
    "batch_size = 8\n",
    "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
    "batch = your_collate_function(raw_batch)\n",
    "\n",
    "'''\n",
    "This is what the 'collate_fn' does in DataLoader\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
    "batch_by_loader = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0a209",
   "metadata": {},
   "source": [
    "#### Pad Sequence and Pack Sequence\n",
    "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
    "- `torch.nn.utils.rnn.pad_sequence`\n",
    "    - This function takes list of tensors with different length and return padded sequence\n",
    "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
    "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
    "    - In default, we use 0 for padding (zero padding)\n",
    "    - The result \n",
    "- `torch.nn.utils.rnn.pack_sequence`\n",
    "    - pad_sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2a21e",
   "metadata": {},
   "source": [
    "Below cells show the example of pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24424475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "short = torch.Tensor([0, 1, 2])\n",
    "long = torch.Tensor([3, 6, 8, 12, 1, 2, 3])\n",
    "middle = torch.Tensor([2, 3, 4, 3, 0])\n",
    "\n",
    "pad_sequence([short, long, middle], batch_first=False)  # T x N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aec137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pad_sequence([short, long, middle], batch_first=True)  # N x T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b420b0",
   "metadata": {},
   "source": [
    "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
    "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
    "\n",
    "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
    "- RNN doesn't know whether it is padded data, or existing data\n",
    "- This makes computation slower\n",
    "\n",
    "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
    "\n",
    "To solve this issue, we use PackedSequence, by using `pack_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553adab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
    "packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c04cfe",
   "metadata": {},
   "source": [
    "`PackedSequence` has `data` and `batch_sizes`\n",
    "- `data` contains the flattened value of given batch\n",
    "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
    "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
    "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
    "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
    "    - `[1,2,0]` means that \n",
    "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
    "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
    "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
    "- `unsorted_indices` shows how the original sequences are sorted.\n",
    "    - `[2,0,1]` means that\n",
    "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences\n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc405b5",
   "metadata": {},
   "source": [
    "If you feed PackedSequence to RNN (or LSTM, GRU), it will return PackedSequence with same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65589f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = torch.nn.GRU(1, 1)\n",
    "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
    "out, last_hidden = rnn_layer(packed_sequence)\n",
    "\n",
    "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
    "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df707e",
   "metadata": {},
   "source": [
    "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
    "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
    "- There are two ways to feed `PackedSequence` to these layers\n",
    "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
    "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e983f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will make error, because other layers cannot handle PackedSequence\n",
    "'''\n",
    "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
    "test_linear_layer(packed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
    "'''\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
    "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
    "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871b5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now you can feed padded sequence to linear layer.\n",
    "'''\n",
    "\n",
    "linear_output = test_linear_layer(padded_sequence)\n",
    "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
    "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d104d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
    "'''\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
    "re_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Another way to do it is using PackedSequence.data\n",
    "'''\n",
    "\n",
    "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
    "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
    "packed_sequence_after_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef97f8e",
   "metadata": {},
   "source": [
    "## Problem 4: Implement pack_collate(), (20 pts)\n",
    "- Implement a collate function that returns PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ed3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "def pack_collate(raw_batch:list):\n",
    "  '''\n",
    "  This function takes a list of data, and returns two PackedSequences\n",
    "  \n",
    "  Argument\n",
    "    raw_batch: A list of MelodyDataset[idx]. Each item in the list is a tuple of (melody, shifted_melody)\n",
    "               melody and shifted_melody has a shape of [num_notes (+1 if you don't consider \"start\" and \"end\" token as note), 2]\n",
    "  Returns\n",
    "    packed_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "    packed_shifted_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "\n",
    "  TODO: Complete this function\n",
    "  '''  \n",
    "  return \n",
    "\n",
    "raw_batch = [train_set[i] for i in range(batch_size)]\n",
    "packed_melody, packed_shifted_melody = pack_collate(raw_batch)\n",
    "packed_melody, packed_shifted_melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test whether you have implemented pack_collate correctly\n",
    "'''\n",
    "\n",
    "assert isinstance(packed_melody, PackedSequence)\n",
    "assert isinstance(packed_shifted_melody, PackedSequence)\n",
    "\n",
    "assert packed_melody.data.shape==packed_shifted_melody.data.shape\n",
    "\n",
    "print(\"Passed all the test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d182eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73263352",
   "metadata": {},
   "source": [
    "## Problem 5: Define Melody Language Model (25 pts)\n",
    "- In this problem, you have to define a Language Model for model\n",
    "    - It is almost same as an ordinary language model for natural language processing\n",
    "    - The key difference is that the melody language model has to predict pitch **and** duration\n",
    "- Complete the model step-by-step\n",
    "    - Complete each function and test the function with the cells below\n",
    "    - `get_concat_embedding()` makes concatenated embedding for each note given pitch and duration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893efba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "class MelodyLanguageModel(nn.Module):\n",
    "  def __init__(self, hidden_size, embed_size, vocabs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embed_size = embed_size\n",
    "    self.num_pitch = len(self.idx2pitch)\n",
    "    self.num_dur = len(self.idx2dur)\n",
    "    self.num_layers = 3\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    TODO: Declare four modules. Please follow the name strictly.\n",
    "      1) self.pitch_embedder: nn.Embedding layer that embed pitch category index to a vector with size of 'embed_size'\n",
    "      2) self.dur_embedder = nn.Embedding layer that embed duration category index to a vector with size of 'embed_size'\n",
    "      3) self.rnn = nn.GRU layer that takes concatenated_embedding and has a hidden size of 'hidden_size', num_layers of self.num_layers, and batch_first=True\n",
    "      4) self.final_layer = nn.Linear layer that takes self.rnn's output and convert it to logits (that can be used as input of softmax) of pitch + duration\n",
    "    '''    \n",
    "    \n",
    "  def get_concat_embedding(self, input_seq):\n",
    "    '''\n",
    "    This function returns concatenated pitch embedding and duration embedding for a given input seq\n",
    "    \n",
    "    Arguments:\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or [num_timesteps (num_notes), 2]\n",
    "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치. \n",
    "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 [타임스텝의 수 (num_notes), 2]\n",
    "    Return:\n",
    "      concat_embedding: A batch of sequence of concatenated embedding of pitch embedding and duration embedding.\n",
    "                        Has a shape of [num_batch, num_timesteps (num_notes), embedding_size * 2]\n",
    "                        Each vector of time t is [pitch_embedding ; duration_embedding] (concatenation)\n",
    "                        \n",
    "                        pitch embedding is the output of an nn.Embedding layer of given note pitch index\n",
    "                        duration embedding is the output of an nn.Embedding layer of given note duration index\n",
    "    \n",
    "    TODO: Complete this function using self.pitch_embedder and self.dur_embedder\n",
    "    You can use torch.cat to concatenate two tensors or vectors\n",
    "    '''\n",
    "    \n",
    "    return \n",
    "  \n",
    "  \n",
    "  def initialize_rnn(self, batch_size: int) -> torch.Tensor :\n",
    "    '''\n",
    "    This function returns initial hidden state for self.rnn for given batch_size\n",
    "    \n",
    "    Argument\n",
    "      batch_size (int): \n",
    "      \n",
    "    Return\n",
    "      initial_hidden_state (torch.Tensor):\n",
    "    '''\n",
    "    \n",
    "    return torch.zeros([self.num_layers, batch_size, self.hidden_size])\n",
    "  \n",
    "    \n",
    "  \n",
    "  def forward(self, input_seq:torch.LongTensor):\n",
    "    '''\n",
    "    Forward propgation of Melody Language Model.\n",
    "    \n",
    "    Argument\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or can be a PackedSequence\n",
    "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치. \n",
    "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 PackedSequence.\n",
    "    \n",
    "    Output\n",
    "      pitch_dist: Probability distribution of pitch of next upcoming note for each timestep 't'.\n",
    "                  Has a shape of [num_batch, numtimesteps, self.num_pitch]\n",
    "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 음고의 확률 분포\n",
    "      dur_dist: Probability distribution of duration of next upcoming note for each timestep 't'.\n",
    "                Has a shape of [num_batch, numtimesteps, self.num_dur]\n",
    "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 길이의 확률 분포\n",
    "      \n",
    "    '''\n",
    "      \n",
    "  \n",
    "    '''\n",
    "    TODO: Complete this function. You have to handle both cases: input_seq as ordinary Tensor / input_seq as PackedSequence\n",
    "    If the input_seq is PackedSequence, return PackedSequence\n",
    "    \n",
    "    \n",
    "    input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration]\n",
    "    \n",
    "    Follow the instruction\n",
    "    '''\n",
    "\n",
    "    if isinstance(input_seq, torch.Tensor): # If input is an ordinary tensor\n",
    "      \n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      \n",
    "      # 2. Put concatenated_embeddings to self.rnn.\n",
    "      # Remember: RNN, GRU, LSTM returns two outputs\n",
    "      \n",
    "      # 3. Put rnn's output with a shape of [num_batch, num_timestep, hidden_size] to self.final_layer\n",
    "      \n",
    "      # 4. Convert logits (output of self.final_layer) to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "\n",
    "      \n",
    "    elif isinstance(input_seq, PackedSequence):      \n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      # To get concatenated_embeddings, You have to either pad_packed_sequence(input_seq, batch_first=True)\n",
    "      # Or use input_seq.data, and then make new PackedSequence using concatenated_embeddings as data, and copy batch_lengths, sorted_indices, unsorted_indices.\n",
    "      \n",
    "      # 2. Put concatenated embedding to self.rnn\n",
    "      \n",
    "      # 3. Put rnn output to self.final_layer to get probability logit for pitch and duration\n",
    "      # Again, rnn's output is PackedSequence so you have to handle it\n",
    "      \n",
    "      # 4. Convert logits to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "      \n",
    "      # Return output as PackedSequence\n",
    "    else:\n",
    "      print(f\"Unrecognized input type: {type(input_seq)}\")\n",
    "    \n",
    "    return\n",
    "  \n",
    "\n",
    "hidden_size = 64\n",
    "embed_size = 40\n",
    "    \n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a8117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Test model.get_concat_embedding\n",
    "'''\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "padded_melody, _ = pad_packed_sequence(melody, batch_first=True)\n",
    "\n",
    "concat_embedding = model.get_concat_embedding(padded_melody)\n",
    "print(f'Your concart_embedding: \\n{concat_embedding}')\n",
    "\n",
    "assert concat_embedding.shape[:-1] == padded_melody.shape[:-1], \"Num_batch and num_timestep of concat_embedding has to be the same with input melody\"\n",
    "assert concat_embedding.shape[2] == embed_size * 2, \"Error in size of embedding dimension\"\n",
    "assert (concat_embedding[0,0,:] == concat_embedding[1,0,:]).all(), \"Error: your embedding vectors for the same input notes are different\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0050c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with ordinary tensor (using batch_size=1)\n",
    "'''\n",
    "\n",
    "single_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "single_batch = next(iter(single_loader))\n",
    "single_melody, single_shifted_melody = single_batch\n",
    "pitch_out, dur_out = model(single_melody)\n",
    "\n",
    "assert pitch_out.shape == (1,single_melody.shape[1], model.num_pitch),  \\\n",
    "          f\"Error in pitch_out.shape. Expected {1,single_melody.shape[1], model.num_pitch}, but got {pitch_out.shape}\"\n",
    "assert dur_out.shape == (1,single_melody.shape[1], model.num_dur), \\\n",
    "          f\"Error in dur_out.shape. Expected {1,single_melody.shape[1], model.num_dur}, but got {dur_out.shape}\"\n",
    "\n",
    "assert (0<pitch_out).all() and (pitch_out<1).all() and (0<dur_out).all() and (dur_out<1).all(), \\\n",
    "          \"Every output must have a value between 0 and 1 \"\n",
    "assert (torch.abs(torch.sum(pitch_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with PackedSequence\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "pitch_out, dur_out = model(melody)\n",
    "\n",
    "assert isinstance(pitch_out, type(melody)) and isinstance(dur_out, type(melody)), f\"Input of model was {type(melody)} but output is {type(pitch_out)}\"\n",
    "\n",
    "assert (pitch_out.batch_sizes == melody.batch_sizes).all(), \\\n",
    "          \"batch_sizes of input and output has to be the same\"\n",
    "assert len(pitch_out.data) == len(batch[0].data), \"Number of notes in input and output has to be the same\"\n",
    "assert (torch.abs(torch.sum(pitch_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734ec23",
   "metadata": {},
   "source": [
    "## Problem 6. Implement training loop (25 pts)\n",
    "- If you have succeeded in implementing model for PackedSequence, you can implement the training loop assuming that input batch is a PackedSequence\n",
    "- If not, you can implement the training loop using batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8d6c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(prob_distribution, correct_class):\n",
    "  '''\n",
    "  This function takes predicted probability distrubtion and the corresponding correct_class.\n",
    "  \n",
    "  For example,  prob_distribution = [[0.2287, 0.2227, 0.5487], [0.1301, 0.4690, 0.4010]] means that\n",
    "  for 0th data sample, the predicted probability for 0th category is 0.2287, for 1st category is 0.2227, and for 2nd category is 0.5487,\n",
    "  and for 1st data sample, the predicted probability for 0th category is 0.1301, for 1st category is 0.4690, and for 2nd category is 0.4010,\n",
    "  \n",
    "  Cross entropy, which is -y*log(y_hat), can be regarded as negative log value of predicted probability for correct class (y==1).\n",
    "  If the given correct_class is [1, 2], the loss for 0th data sample becomes negative log of [0.2287, 0.2227, 0.5487][1], which is -torch.log(0.2227), \n",
    "  because the correct category for this sample was 1st category, and the predicted probability was 0.2227\n",
    "  And the loss for 1st data sample becomes negative log of [0.1301, 0.4690, 0.4010][2], which is -torch.log(0.4010),\n",
    "  because the correct category for this sample was 2nd category, and the predicted probability was 0.4010\n",
    "  \n",
    "  To make implementation easy, let's assume we have 2D tensor for prob_distribution and  1D tensor for correct_class\n",
    "   \n",
    "  Arguments:\n",
    "    prob_distribution (2D Tensor)\n",
    "    correct_class (1D Tensor)\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Cross entropy loss for every data sample in prob_distrubition. Has a same shape with correct_class\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  \n",
    "  Caution: When use torch.log(), don't forget to add small epsilon value (like 1e-6) to avoid infinity\n",
    "  Do not return the mean loss. Return loss that has same shape with correct_class\n",
    "  Try not to use for loop, or torch.nn.CrossEntropyLoss, or torch.nn.NLLLoss\n",
    "  '''\n",
    "  assert prob_distribution.dim() == 2 and correct_class.dim() == 1, \"Let's assume we only take 2D tensor for prob_distribution and 1D tensor for correct_class\"\n",
    "  # Write your code from here\n",
    "  \n",
    "  return\n",
    "torch.manual_seed(0)\n",
    "prob_distribution = torch.softmax(torch.randn([10, 3]), dim=-1)\n",
    "correct_class = torch.randint(0,3, [10])\n",
    "print(f\"prob_distribution: \\n{prob_distribution}, \\n correct_class for each datasample: \\n {correct_class.unsqueeze(1)}\")\n",
    "\n",
    "loss = get_cross_entropy_loss(prob_distribution, correct_class)\n",
    "print('Loss: ', loss)\n",
    "assert (torch.abs(loss-torch.Tensor([1.5020, 0.7572, 0.4797, 0.7693, 0.4563, 0.8718, 0.7973, 1.3412, 1.6403, 0.2423]))<1e-4).all(), \"Error in loss value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963172fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_for_single_batch(model, batch, device):\n",
    "  '''\n",
    "  This function takes model and batch and calculate Cross Entropy Loss for given batch.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    batch (batch collated by pack_collate): Tuple of (melody_batch, shifted_melody_batch)\n",
    "    device (str): cuda or cpu. In which device to calculate the batch\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Calculated mean loss for given model and batch\n",
    "    \n",
    "  TODO: Complete this function using get_cross_entropy_loss().\n",
    "  Now you have to return the mean loss of every data sample in the batch \n",
    "  \n",
    "  Caution: You have to calculate loss for pitch, and loss for duration separately.\n",
    "  Then you can take average of pitch_loss and duration_loss\n",
    "  \n",
    "  Important Tip: If you are using PackedSequence, you can feed PackedSequence.data directly to get_cross_entropy_loss.\n",
    "  It makes the implementation much easier, because it doesn't need to reshape probabilty distribution to 2D and correct class to 1D.\n",
    "  '''\n",
    "  \n",
    "\n",
    "  return\n",
    "\n",
    "model.to('cuda')\n",
    "batch = next(iter(train_loader))\n",
    "get_loss_for_single_batch(model, batch, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you have implemented the previous function correctly, this code will train the model\n",
    "'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEV = 'cuda' # or cpu, but using cpu will be too slow\n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 30\n",
    "\n",
    "model.to(DEV)\n",
    "loss_record = []\n",
    "valid_loss_record = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "  model.train()\n",
    "  for batch in train_loader:\n",
    "    loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_record.append(loss.item())\n",
    "    \n",
    "  # Validation\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    loss_for_entire_valid = 0\n",
    "    num_notes = 0\n",
    "    for batch in valid_loader:\n",
    "      loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "      if isinstance(batch[0], PackedSequence):\n",
    "        n_note = len(batch[0].data)        \n",
    "      else:\n",
    "        n_note = batch[0].shape[1]\n",
    "        \n",
    "      loss_for_entire_valid += loss.item() * n_note\n",
    "      num_notes += n_note\n",
    "    valid_loss_record.append(loss_for_entire_valid/num_notes)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be25b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Training Loss')\n",
    "plt.plot(loss_record)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Validation Loss')\n",
    "plt.plot(valid_loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e96ed2",
   "metadata": {},
   "source": [
    "## Problem 7: Implement Generation (25 pts)\n",
    "- In this problem, you have to generate a new melody using the trained model\n",
    "- Melody language model can generate a new sequence by sampling a new note for each timestep, and feed the generated new note again to the model to predict the next note\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd915d",
   "metadata": {},
   "source": [
    "### Problem 7-1: Implement model inference (15 pts)\n",
    "- Inference in Language model is little bit different from an ordniary forward loop during the training.\n",
    "    - While training, you have entire sequence, from beginning to end.\n",
    "    - During the inference, you have to generate one note, and then feed it as an input for the next step\n",
    "- You have to implement given functions one by one to complete `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_input_and_hidden_state(model, batch_size=1):\n",
    "  '''\n",
    "  This function generates initial input vector and hidden state for model's GRU\n",
    "  \n",
    "  To generate a new sequence, you have to provide initial seed token, which is ['start', 'start'].\n",
    "  You have to make a initial vector that has [pitch_category_index_of_'start', duration_category_index_of_'start']\n",
    "  \n",
    "  You also have to initial hidden state for the model's RNN.\n",
    "  In uni-directional RNN(or GRU), hidden state of RNN has to be a zero tensor with shape of (num_layers, batch_size, hidden_size)\n",
    "\n",
    "  \n",
    "  Argument:\n",
    "    model (MelodyLanguageModel)\n",
    "    \n",
    "  Returns:\n",
    "    initial_input_vec (torch.Tensor): Has a shape of [batch_size, 1 (timestep), 2]\n",
    "    initial_hidden (torch.Tensor): Has a shape of [num_layers, bach_size, hidden_size]\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  \n",
    "  return\n",
    "\n",
    "batch_size = 2\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=batch_size)\n",
    "print(f'input_vec: \\n{input_vec} \\n initial_hidden: \\n {initial_hidden}')\n",
    "\n",
    "assert input_vec.ndim == 3\n",
    "assert initial_hidden.ndim == 3\n",
    "assert input_vec.shape == (batch_size, 1, 2)\n",
    "assert initial_hidden.shape == (model.num_layers, batch_size, model.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c5e91",
   "metadata": {},
   "source": [
    "### Hint: Sampling from distribution\n",
    "- The language model predict probability distribution of pitch and duration for  upcoming note\n",
    "- To do that, you have to know how to sample a result from a given probability distribution\n",
    "- In PyTorch, you can use `atensor.multinomial(num_samples)`\n",
    "    - In this assignment you don't have to sample more than 1, but \n",
    "    - multinomial(num_samples=100, replacement=False) means that you want to sample 100 samples without overlapping category\n",
    "        - Thus, the total class has to be larger than 100, because you cannot sample a single category multiple time\n",
    "    - multinomial(num_samples=100, replacement=True) means that you will sample 100 from the distrubtion independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23845568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "'''\n",
    "Example of sampling a result from a given probability distribution\n",
    "'''\n",
    "\n",
    "dummy_prob_distribution = torch.Tensor([0.1, 0.5, 0.2, 0.05, 0.15])\n",
    "sampled_out = dummy_prob_distribution.multinomial(num_samples=10000, replacement=True)\n",
    "print(sampled_out[:20])\n",
    "Counter(sampled_out.tolist()) # Number of each category sampled is almost same as num_samples * probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fef28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_single_step(model, cur_input, prev_hidden):\n",
    "  '''\n",
    "  This function runs MelodyLangaugeModel just for one step, for the given current input and previous hidden state.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_input (torch.LongTensor): Input for the current time step. Has a shape of (batch_size=1, 1 (timestep), 2)\n",
    "    prev_hidden (torch.Tensor): Hidden state of RNN after previous timestep\n",
    "\n",
    "  Returns:\n",
    "    cur_output (torch.LongTensor): Sampled note [pitch_category_idx, duration_category_idx] from the predicted probability distribution, with shape of [1,1,2]\n",
    "    last_hidden (torch.Tensor): Hidden state of RNN\n",
    "  Think about running the model.forward() step-by-step.\n",
    "  \n",
    "  input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration] → sampled [pitch, duration]\n",
    "\n",
    "  '''\n",
    "  return \n",
    "\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=1)\n",
    "out_note, last_hidden = predict_single_step(model, input_vec, initial_hidden)\n",
    "print(f'out_note: \\n{out_note} \\n last_hidden: \\n {last_hidden}')\n",
    "\n",
    "assert out_note.ndim == 3\n",
    "assert last_hidden.ndim == 3\n",
    "assert out_note.shape == (1,1,2)\n",
    "\n",
    "assert len(set([predict_single_step(model, input_vec, initial_hidden)[0] for i in range(5)]))==5, 'Generated output has to be different based on random sampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_end_token(model, cur_output):\n",
    "  '''\n",
    "  During the generation, there is a possibility that the generated note predicted 'end' token for either pitch or duration.\n",
    "  (In fact, model can even estimate 'start' token during the generation even though it has very low probability)\n",
    "  \n",
    "  Using information among (model.pitch2idx, model.dur2idx, model.idx2pitch, model.idx2dur, model.num_pitch, model.num_dur), check whether \n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_output (torch.LongTensor): Assume it has shape of [1,1,2 (pitch_idx, duration_idx)]\n",
    "  \n",
    "  Return:\n",
    "    is_end_token (bool): True if cur_output include category index such as 'start' or 'end',\n",
    "                          else False.\n",
    "                          \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  \n",
    "  \n",
    "  return \n",
    "\n",
    "\n",
    "print(is_end_token(model, out_note))\n",
    "\n",
    "assert not is_end_token(model, torch.LongTensor([[[10, 7]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[38, 40]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[25, 44]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[39, 41]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079ad9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "def generate(model, random_seed=0):\n",
    "  '''\n",
    "  This function generates a new melody sequence with a given model and random_seed.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    random_seed (int): Language model's inference will always generate different result, because it uses random sampling for the prediction.\n",
    "                       Therefore, if you want to reproduce the same generation result, you have to fix random_seed.\n",
    "  \n",
    "  Returns:\n",
    "    generated_note_sequence (torch.LongTensor): Has a shape of [num_generated_notes, 2]\n",
    "  \n",
    "  TODO: Complete this function using get_initial_input_and_hidden_state(), predict_single_step(), is_end_token()\n",
    "  \n",
    "  Hint: You can use while loop\n",
    "        You have to track the generated single note in a list or somewhere. \n",
    "  '''\n",
    "  \n",
    "  torch.manual_seed(random_seed) # To reproduce the result, we have to control random sequence\n",
    "  \n",
    "  '''\n",
    "  Write your code from here\n",
    "  '''\n",
    "\n",
    "  return\n",
    "gen_out = generate(model)\n",
    "print(f\"gen_out: \\n {gen_out}\")\n",
    "\n",
    "assert isinstance(gen_out, torch.LongTensor), f\"output of generate() has to be torch.LongTensor, not {type(gen_out)}\"\n",
    "assert gen_out.ndim == 2, f\"output of generate() has to be 2D tensor, not {gen_out.ndim}D tensor\"\n",
    "assert gen_out.shape[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787bda",
   "metadata": {},
   "source": [
    "### Problem 7-2. Convert neural network's prediction to music score (10 pts)\n",
    "- Even though neural network has succeeded in generating a new sequence, it is just a sequence of index that neural network uses\n",
    "    - For example, generated note event [17, 10] means that this note has pitch value of 17th pitch category and duration value of 10th duration category\n",
    "- We have to convert categorical index to original value\n",
    "    - We saved this information as `idx2pitch`, `idx2dur` while we declared the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_pred_to_origin(pred:torch.Tensor, idx2pitch:list, idx2dur:list):\n",
    "  '''\n",
    "  This function convert neural net's output index to original pitch value (MIDI Pitch) and duration value \n",
    "  \n",
    "  Argument:\n",
    "    pred: generated output of the model. Has a shape of [num_notes, 2]. \n",
    "          0th dimension of each note represents pitch category index \n",
    "          and 1st dimension of each note represents duration category index\n",
    "  \n",
    "  Return:\n",
    "    converted_out (torch.Tensor): Has a same shape with 'pred'.\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "    \n",
    "  return \n",
    "\n",
    "converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "assert converted_out.shape == gen_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be29831",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To solve the next problem, you have to know how note_representation looks like in muspy.\n",
    "\n",
    "In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "'''\n",
    "\n",
    "note_repr_example = train_set.dataset[0]\n",
    "note_repr_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067efdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_pitch_dur_to_note_representation(pitch_dur:torch.LongTensor) -> :\n",
    "  '''\n",
    "  This function takes pitch_dur (shape of [num_notes, 2]) and returns the corresponding note representation (shape of [num_notes, 4])\n",
    "  In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "  \n",
    "  Since our generation is monophonic, you can regard start_timestep starts from 0 and accumulate the duration of note.\n",
    "  You can fix velocity to 64.\n",
    "  \n",
    "  \n",
    "  Arguments:\n",
    "    pitch_dur: LongTensor of note where each note represented as pitch and duration value\n",
    "    \n",
    "  return:\n",
    "    note_repr: numpy.Array with shape of [num_notes, 4]\n",
    "               each note has value of [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "  TODO: Complete this function\n",
    "  Hint: You can use torch.cumsum() to accumulate the duration.\n",
    "  To convert torch tensor to numpy, you can use atensor.numpy()\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  return \n",
    "\n",
    "note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "note_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d9303",
   "metadata": {},
   "source": [
    "## Generation: Visualize and synthesize the generated result (10 pts)\n",
    "- Try to generate different melody using different `random_seed`\n",
    "- In your submission, include **Three** examples of your favorite among the generated results in wav\n",
    "    - You have to install soundfont and music font using \n",
    "        - `muspy.download_bravura_font()`\n",
    "        - `muspy.download_musescore_soundfont()`\n",
    "    - You may need fluidsynth to synthesize the sound.\n",
    "        - In colab, `!sudo apt-get install fluidsynth` will work\n",
    "        - In other Ubuntu os, `sudo apt-get update` and then `sudo apt-get install fluidsynth` will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71a3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "gen_music = muspy.from_note_representation(note_repr)\n",
    "gen_music.show_score()\n",
    "\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9a3cb",
   "metadata": {},
   "source": [
    "- Try with different random seed and generate interesting melodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_muspy_music(model, random_seed=0):\n",
    "  '''\n",
    "  This function combines 'generate', 'convert_idx_pred_to_origin', 'convert_pitch_dur_to_note_representation', muspy.from_note_representation\n",
    "  '''\n",
    "  gen_out = generate(model, random_seed)\n",
    "  converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "  note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "  gen_music = muspy.from_note_representation(note_repr)\n",
    "  return gen_music\n",
    "\n",
    "gen_music = generate_muspy_music(model, random_seed=2)\n",
    "gen_music.show_score()\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
